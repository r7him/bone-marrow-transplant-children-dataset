# -*- coding: utf-8 -*-
"""BoneMarrow.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/17MPLL4LuplrfIaGdpRq5viQmwTECaOVp

> Indented block

Bone marrow transplant: children Data Set
Abstract: The data set describes pediatric patients with several hematologic diseases, who were subject to the unmanipulated allogeneic unrelated donor hematopoietic stem cell transplantation.
	

*   Data Set Characteristics:  Multivariate

*   Number of Instances:187

*   Area:Life

*   Attribute Characteristics:Integer, Real
*   Number of Attributes:39


*   Date Donated:2020-04-21


*   Associated Tasks:Classification, Regression


*   Missing Values:Yes

Link: [Bone marrow transplant: children Data Set](https://archive.ics.uci.edu/ml/datasets/Bone+marrow+transplant%3A+children)

Data Set Information:

The data set describes pediatric patients with several hematologic diseases: malignant disorders (i.a. acute lymphoblastic leukemia, acute myelogenous leukemia, chronic myelogenous leukemia, myelodysplastic syndrome) and nonmalignant cases (i.a. severe aplastic anemia, Fanconi anemia, with X-linked adrenoleukodystrophy). All patients were subject to the unmanipulated allogeneic unrelated donor hematopoietic stem cell transplantation.

The motivation of the study was to identify the most important factors influencing the success or failure of the transplantation procedure. In particular, the aim was to verify the hypothesis that increased dosage of CD34+ cells / kg extends overall survival time without simultaneous occurrence of undesirable events affecting patients' quality of life (KawÅ‚ak et al., 2010).

The data set has been used in our work concerning survival rules (WrÃ³bel et al., 2017) and user-guided rule induction (Sikora et al., 2019). The authors of the research on stem cell transplantation (KawÅ‚ak et al., 2010) who inspired our study also contributed to the set.
"""

#####Important Libraries
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import plotly.express as px

"""*Importing* Dataset & Data preprocessing"""

#Importing Dataset
df = pd.read_csv('/content/drive/MyDrive/Bone Marrow/bone-marrow.csv')
df.head()

df.shape

df.dtypes

df.columns

"""Data Cleaning

"""

####A brief of data####
df.describe()

#selected_feat

#selected_columns

df.columns

"""A brief of whole data"""

#####Dropping unnecesary column#######
#recipient_age_below_10, recipient_age_int, gender_match, HLA_mismatch, antigen, ', 'HLA_group_1', acute_GvHD_II_III_IV, acute_GvHD_III_IV

df.drop(['id','recipient_age_below_10','recipient_age_int','gender_match','HLA_mismatch','antigen','acute_GvHD_II_III_IV','acute_GvHD_III_IV'],axis=1, inplace=True)

df

"""

```

```

**Dropping redundant column  as they are not requiered**"""

#New Dataframe
df.shape

df.nunique()

df['ABO_match'].unique()

"""We will remove '?' later"""

df['recipient_ABO'].unique()



######### Separating Catagorical & Neumerical Column#########
def extract_cat_num(df):
    cat_col=[col for col in df.columns if df[col].dtype=='object']
    num_col=[col for col in df.columns if df[col].dtype!='object']
    return cat_col,num_col

"""Separating numerical & categorical data from main dataframe"""

cat_col,num_col=extract_cat_num(df)

cat_col

num_col

for col in cat_col:
    print('{} has {} values '.format(col,df[col].unique()))
    print('\n')

"""Some neumerical data is in the cat_col. We will add this in num_col"""

#coverting data_types
def convert_dtype(df,feature):
    df[feature] = pd.to_numeric(df[feature],errors='coerce')

features =['CD3_x1e8_per_kg','CD3_to_CD34_ratio','CD3_x1e8_per_kg','recipient_body_mass','CD3_to_CD34_ratio','survival_status']
for feature in features:
    convert_dtype(df,feature)

###############Again finding cat column and num column to get the exatc info###########
def extract_cat_num(df):
    cat_col=[col for col in df.columns if df[col].dtype=='object']
    num_col=[col for col in df.columns if df[col].dtype!='object']
    #num_col=[col for col in df.columns if df[col].dtype=='float64']
    return cat_col,num_col
cat_col,num_col=extract_cat_num(df)

cat_col

len(cat_col)

num_col

len(num_col)

for col in cat_col:
    print('{} has {} values '.format(col,df[col].unique()))
    print('\n')

"""Now this cat_col has only categorical data"""

###########Checking NUll values############
df.isnull().sum()





for col in num_col:
    print('{} has {} values '.format(col,df[col].unique()))
    print('\n')

"""This rows num_col has some 'nan' values"""

df[df['CD3_to_CD34_ratio'].isnull()]

"""List of null in this dataframe"""





df[df['CD3_to_CD34_ratio'].isnull()].index

df.info()

##########Ploting Data##########
plt.figure(figsize=(30,20))

for i, feature in enumerate(num_col):
    plt.subplot(5,3,i+1)
    df[feature].hist()
    plt.title(feature)



len(cat_col)

sns.countplot(df['recipient_gender'])

sns.countplot(df['stem_cell_source'])

df.corr()

########## Correlation shows which is correalted with which column############
plt.figure(figsize=(10,8))
df.corr()
sns.heatmap(df.corr(),annot=True)

sns.lmplot(x='donor_age', y='recipient_age', data=df)

sns.relplot(x='donor_age',y='recipient_age', hue = 'stem_cell_source',data=df)

sns.relplot(x='survival_time',y='recipient_age', hue = 'recipient_gender',data=df)

sns.distplot(df['recipient_age'])

px.violin(df,y= 'recipient_age',x ='disease_group')

df.columns

px.scatter(df,x='donor_age', y =  'recipient_age')

sns.swarmplot(x='ANC_recovery', y='PLT_recovery', data=df, 
              hue='disease')

grid = sns.FacetGrid(df,hue='recipient_gender',aspect = 2)
grid.map(sns.kdeplot,'survival_time')
grid.add_legend()

##### Showing info about disease and recipent age##########
grid = sns.FacetGrid(df,hue='disease',aspect = 2)
grid.map(sns.kdeplot,'recipient_age')
grid.add_legend()

sns.kdeplot(df.recipient_body_mass, df.survival_time)

##############Scatter Plot##############
fig = px.scatter(df, x="recipient_body_mass", y="recipient_age", color="survival_status")
fig.show()

fig = px.scatter(df, x="recipient_body_mass", y="survival_time", color="recipient_gender")
fig.show()

#########Box and Scatter#############
fig = px.scatter(df, x="donor_age", y="recipient_age", color="disease", marginal_y="violin",
           marginal_x="box", trendline="ols", template="simple_white")
fig.show()

df.dtypes

############Describe this#############
fig = px.scatter_matrix(df, dimensions=["donor_age", "recipient_age","CD3_to_CD34_ratio",'survival_time'], color="risk_group")
fig.show()





df.columns

df.dtypes

"""#############
Creating functions to automate our EDA
"""

def violin(col):
    fig=px.violin(df,y=col,x='survival_status',color = 'disease',box=True)
    return fig.show()

def scatters(col1,col2):
    fig = px.scatter(df,x=col1,y=col2,color='survival_status')
    return fig.show()

def kde_plot(feature):   
    grid = sns.FacetGrid(df,hue='survival_status',aspect = 2)
    grid.map(sns.kdeplot,feature)
    grid.add_legend()

kde_plot('survival_time')

scatters('recipient_age','survival_time')

violin('HLA_match')

violin('recipient_body_mass')

# boxplot:
plt.figure(figsize=(12, 8))
sns.boxplot(x="recipient_age",
            y="disease",
            hue="survival_status",
            data=df)
plt.legend(loc="upper right")
plt.show()

df.info()



"""Previously we obserevd that we had some nan values.Data Cleaning for missing values"""

df.isnull().sum().sort_values(ascending = True)

df['CD3_x1e8_per_kg'].isnull().sum()

df.dropna(inplace=True)

df[num_col].isnull().sum()

df[cat_col].isnull().sum()

###############Data Is cleaned fully

"""We have completed our EDA. Now we will look into feature importance & will 

*   List item
*   List item

selec the prediction model.
"""



from sklearn.preprocessing import LabelEncoder

le = LabelEncoder()
data = df

for col in cat_col:
    data[col]=le.fit_transform(data[col])

data.head()

from sklearn.feature_selection import SelectKBest
from sklearn.feature_selection import chi2

ind_col =[col for col in data.columns if col!='survival_status']
dep_col='survival_status'

X= data[ind_col]
y = data[dep_col]

X.head()

y

ordered_rank_features = SelectKBest(score_func=chi2,k=20)
ordered_feature = ordered_rank_features.fit(X,y)

ordered_feature

ordered_feature.scores_

datascores = pd.DataFrame(ordered_feature.scores_,columns=['Score'])

datascores

dfcols = pd.DataFrame(X.columns)
dfcols

features_rank = pd.concat([dfcols,datascores],axis=1)
features_rank

features_rank.nlargest(10,'Score')

selected_columns = features_rank.nlargest(10,'Score')[0].values

selected_columns

X_new = data[selected_columns]
X_new.head()

len(X_new)

X_new.shape

from sklearn.model_selection import train_test_split

X_train,X_test, y_train,y_test =  train_test_split(X_new,y,random_state=109,test_size=0.25)



"""Using XGboost classsifier"""

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X_new,y,train_size=0.75)

print(X_train.shape)
print(X_test.shape)

y_train.value_counts()

from xgboost import XGBClassifier
XGBClassifier()

params={
 "learning_rate"    : [0.05, 0.20, 0.25 ] ,
 "max_depth"        : [ 5, 8, 10, 12],
 "min_child_weight" : [ 1, 3, 5, 7 ],
 "gamma"            : [ 0.0, 0.1, 0.2 , 0.4 ],
 "colsample_bytree" : [ 0.3, 0.4, 0.7 ]
    
}

from sklearn.model_selection import RandomizedSearchCV

from xgboost import XGBClassifier
classifier=XGBClassifier()

import warnings
from warnings import filterwarnings
filterwarnings('ignore')

random_search=RandomizedSearchCV(classifier,param_distributions=params,n_iter=5,scoring='roc_auc',n_jobs=-1,cv=5,verbose=3)

random_search.fit(X_train, y_train)

random_search.best_estimator_

random_search.best_params_

classifier = XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=0.3, gama=0.0, gamma=0,
              gpu_id=0, importance_type='gain', interaction_constraints='',
              learning_rate=0.05, max_delta_step=0, max_depth=5,
              min_child_weight=1, monotone_constraints='0',
              n_estimators=100, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=0, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=1,
              tree_method='exact', use_label_encoder=True,
              validate_parameters=1, verbosity=0)

## we have got this model on the basis of cross valudation & hyper-parameter optimization

classifier.fit(X_train,y_train)

y_pred=classifier.predict(X_test)

from sklearn.metrics import confusion_matrix,accuracy_score

confusion = confusion_matrix(y_test, y_pred)
print('Confusion Matrix:')
print(confusion)

plt.imshow(confusion)

#######From XGboost accuracy
accuracy_score(y_test, y_pred)



"""Using LInear Regression"""

from sklearn.linear_model import Lasso
from sklearn.feature_selection import SelectFromModel

feature_sel_model = SelectFromModel(Lasso(alpha=0.005, random_state=0)) # remember to set the seed, the random state in this function
feature_sel_model.fit(X,y)

feature_sel_model.get_support()

cols=X.columns

selected_feat = cols[(feature_sel_model.get_support())]

selected_feat

# let's print some stats
print('total features: {}'.format((X.shape[1])))
print('selected features: {}'.format(len(selected_feat)))

# let's print some stats
print('total features: {}'.format((X.shape[1])))
print('selected features: {}'.format(len(selected_feat)))

X=X[selected_feat]

from sklearn.model_selection import train_test_split
X_train,X_test,y_train,y_test=train_test_split(X,y,train_size=0.75,random_state=0)

from sklearn.linear_model import LogisticRegression
logreg=LogisticRegression()
logreg.fit(X_train,y_train)

y_pred=logreg.predict(X_test)

from sklearn.metrics import confusion_matrix
cm=confusion_matrix(y_test,y_pred)
cm

###########From Linear regression
from sklearn.metrics import accuracy_score
score=accuracy_score(y_test,y_pred)
score



"""Cross Validation with other models 

"""

from sklearn.model_selection import cross_val_score
score=cross_val_score(logreg,X,y,cv=10)

score

score.mean()

#fit naive bayes
from sklearn.naive_bayes import GaussianNB
from sklearn.linear_model import LogisticRegression
from sklearn.neighbors import KNeighborsClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.tree import DecisionTreeClassifier

### classifier models
models = []
models.append(('LogisticRegression', LogisticRegression()))
models.append(('Naive Bayes',GaussianNB()))
models.append(('RandomForest', RandomForestClassifier()))
models.append(('Decision Tree', DecisionTreeClassifier()))
models.append(('KNN', KNeighborsClassifier(n_neighbors = 5)))

for name, model in models:
    print(name)
    model.fit(x_train, y_train)
    
    # Make predictions.
    predictions = model.predict(x_test)

    # Compute the error.
    from sklearn.metrics import confusion_matrix
    print(confusion_matrix(predictions, y_test))

    from sklearn.metrics import accuracy_score
    print(accuracy_score(predictions,y_test))
    print('\n')

##########Cross Validation#### of Accuracy########



"""MOdel Accuracy: 

1.   XgbClasifier: 93%
2.   Logistic Regression: 89%
3.   Naive Bayes:85%
4.   RandomForest: 89%
5.   Decision Tree: 84%
6.   KNN: 93%


"""

